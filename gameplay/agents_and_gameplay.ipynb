{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "\n",
    "#https://python-chess.readthedocs.io/en/latest/\n",
    "import chess, random\n",
    "from mcts import mcts\n",
    "from chess import Move, svg\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from mcts import mcts\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK = -1\n",
    "WHITE = 1\n",
    "\n",
    "class ChessState():\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.currentPlayer = 1\n",
    "    def getCurrentPlayer(self):\n",
    "        # 1 for maximiser, -1 for minimiser\n",
    "        return self.currentPlayer\n",
    "\n",
    "    def getLegalActions(self):\n",
    "        legal_actions = list(self.board.legal_moves)\n",
    "        result = []\n",
    "        for a in legal_actions:\n",
    "            result.append(Action(a.uci()))\n",
    "        return result\n",
    "\n",
    "    def generateSuccessor(self, action):\n",
    "        if action is not None:\n",
    "        \n",
    "            newState = deepcopy(self)\n",
    "            newState.board.push(Move.from_uci(action.uci))\n",
    "            newState.currentPlayer = -1 * newState.currentPlayer\n",
    "            return newState\n",
    "        return self\n",
    "\n",
    "    def isTerminal(self):\n",
    "        return self.board.is_game_over()\n",
    "    \n",
    "    def isWin(self):\n",
    "        if self.isTerminal():\n",
    "            res = self.board.result()\n",
    "            if res == \"1-0\":\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def isLose(self):\n",
    "        if self.isTerminal():\n",
    "            return not self.isWin()\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    #DRAWS\n",
    "    \n",
    "    def getReward(self):\n",
    "        # only needed for terminal states\n",
    "        if self.isTerminal():\n",
    "            res = self.board.result()\n",
    "            if res == \"1-0\":\n",
    "                return 1\n",
    "            elif res == \"0-1\":\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action():\n",
    "    def __init__(self, uci):\n",
    "        self.uci = uci\n",
    "    def __hash__(self):\n",
    "        return hash(self.uci)\n",
    "    def __str__(self):\n",
    "        return self.uci\n",
    "    def __repr__(self):     \n",
    "        return self.uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def getAction(self, possible_actions):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def getAction(self, possible_actions):\n",
    "        return random.choice(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ReflexAgent(Agent):\n",
    "    \"\"\"\n",
    "    A reflex agent chooses an action at each choice point by examining\n",
    "    its alternatives via a state evaluation function.\n",
    "    The code below is provided as a guide.  You are welcome to change\n",
    "    it in any way you see fit, so long as you don't touch our method\n",
    "    headers.\n",
    "    \"\"\"\n",
    "\n",
    "    def getAction(self, gameState):\n",
    "        \"\"\"\n",
    "        You do not need to change this method, but you're welcome to.\n",
    "        getAction chooses among the best options according to the evaluation function.\n",
    "        Just like in the previous project, getAction takes a GameState and returns\n",
    "        some Directions.X for some X in the set {NORTH, SOUTH, WEST, EAST, STOP}\n",
    "        \"\"\"\n",
    "        # Collect legal moves and successor states\n",
    "        legalMoves = gameState.getLegalActions()\n",
    "\n",
    "        # Choose one of the best actions\n",
    "        scores = [self.evaluationFunction(gameState, action) for action in legalMoves]\n",
    "        bestScore = max(scores)\n",
    "        bestIndices = [index for index in range(len(scores)) if scores[index] == bestScore]\n",
    "        chosenIndex = random.choice(bestIndices)  # Pick randomly among the best\n",
    "\n",
    "        \"Add more of your code here if you want to\"\n",
    "\n",
    "        return legalMoves[chosenIndex]\n",
    "\n",
    "    def evaluationFunction(self, currentGameState, action):\n",
    "    \n",
    "        return scoreEvaluationFunction(currentGameState.generateSuccessor(action))\n",
    "    #         [1, 3, 3, 5, 9, 0]\n",
    "        \n",
    "\n",
    "\n",
    "def scoreEvaluationFunction(currentGameState):\n",
    "    \"\"\"\n",
    "    This default evaluation function just returns the score of the state.\n",
    "    The score is the same one displayed in the Pacman GUI.\n",
    "    This evaluation function is meant for use with adversarial search agents\n",
    "    (not reflex agents).\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    fen = currentGameState.board.fen()\n",
    "    piece_values = {'p': -1, 'P': 1, 'k': 0, 'K': 0, 'n': -3, 'N':3, 'b': -3, 'B':3, 'r': -5, 'R':5, 'q': -9, 'Q': 9}\n",
    "    \n",
    "    def get_value(char):\n",
    "        try:\n",
    "            return piece_values[char]\n",
    "        except:\n",
    "            return 0\n",
    "    for i in range(fen.index(\" \")):\n",
    "        score+=get_value(fen[i])\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "class MultiAgentSearchAgent(Agent):\n",
    "    \"\"\"\n",
    "    This class provides some common elements to all of your\n",
    "    multi-agent searchers.  Any methods defined here will be available\n",
    "    to the MinimaxPacmanAgent, AlphaBetaPacmanAgent & ExpectimaxPacmanAgent.\n",
    "    You *do not* need to make any changes here, but you can if you want to\n",
    "    add functionality to all your adversarial search agents.  Please do not\n",
    "    remove anything, however.\n",
    "    Note: this is an abstract class: one that should not be instantiated.  It's\n",
    "    only partially specified, and designed to be extended.  Agent (game.py)\n",
    "    is another abstract class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, evalFn=scoreEvaluationFunction, depth='3'):\n",
    "        self.index = 0  # Pacman is always agent index 0\n",
    "        self.evaluationFunction = evalFn\n",
    "        self.depth = int(depth)\n",
    "\n",
    "\n",
    "class MinimaxAgent(MultiAgentSearchAgent):\n",
    "    \n",
    "    def min_value(self,state, depth):\n",
    "        v = float(\"inf\")\n",
    "        best_action_indices = []\n",
    "        actions = state.getLegalActions()\n",
    "\n",
    "        if state.isWin() or state.isLose():\n",
    "            return self.evaluationFunction(state), None\n",
    "\n",
    "        if depth >= self.depth:\n",
    "            for action_index in range(len(actions)):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.evaluationFunction(successor_state)\n",
    "                if successor_value < v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "                 \n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        else:\n",
    "            for action_index in range(len(actions)):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.max_value(successor_state, depth + 1)[0]\n",
    "                if successor_value < v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "                    \n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "\n",
    "    def max_value(self,state, depth):\n",
    "        v = float(\"-inf\")\n",
    "        best_action_indices = []\n",
    "        actions = state.getLegalActions()\n",
    "        if state.isWin() or state.isLose() :\n",
    "            return self.evaluationFunction(state), None\n",
    "        if depth >= self.depth :\n",
    "\n",
    "            for action_index in range(len(actions)):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.evaluationFunction(successor_state)\n",
    "                if successor_value > v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        else :\n",
    "            for action_index in range(len(actions)):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.min_value(successor_state, depth)[0]\n",
    "                if successor_value > v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        \n",
    "    def getAction(self, gameState):\n",
    "        if gameState.currentPlayer == 1 :\n",
    "            return self.max_value(gameState, 0)[1]\n",
    "        if gameState.currentPlayer == -1 :\n",
    "            return self.min_value(gameState, 0)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaAgent(MultiAgentSearchAgent):\n",
    "    \"\"\"\n",
    "    Your minimax agent with alpha-beta pruning (question 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def min_value(self,state, depth, alpha, beta):\n",
    "        v = float(\"inf\")\n",
    "        best_action_indices = [0]\n",
    "        actions = state.getLegalActions()\n",
    "        #actions = random.sample(actions, len(actions)//2)\n",
    "        #print(len(actions))\n",
    "\n",
    "        if state.isWin() or state.isLose():\n",
    "            return self.evaluationFunction(state), None\n",
    "\n",
    "        if depth >= self.depth:\n",
    "            for action_index in random.sample(range(len(actions)), k = len(actions)//4):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.evaluationFunction(successor_state)\n",
    "                if successor_value < v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "                 \n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        else:\n",
    "            currentBeta = beta\n",
    "            for action_index in random.sample(range(len(actions)), k = len(actions)//4):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.max_value(successor_state, depth + 1, alpha, currentBeta)[0]\n",
    "                if successor_value < v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "                if v < alpha:\n",
    "                    return v, actions[random.choice(best_action_indices)]\n",
    "                currentBeta = min(currentBeta, v)\n",
    "                    \n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "\n",
    "    def max_value(self,state, depth, alpha, beta):\n",
    "        v = float(\"-inf\")\n",
    "        best_action_indices = [0]\n",
    "        actions = state.getLegalActions()\n",
    "        \n",
    "        #actions = random.sample(actions, len(actions)//2)\n",
    "        #print(len(actions))\n",
    "\n",
    "        if state.isWin() or state.isLose() :\n",
    "            return self.evaluationFunction(state), None\n",
    "        if depth >= self.depth :\n",
    "            for action_index in random.sample(range(len(actions)), k = len(actions)//4):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.evaluationFunction(successor_state)\n",
    "                if successor_value > v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "    \n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        else :\n",
    "            currentAlpha = alpha\n",
    "            for action_index in random.sample(range(len(actions)), k = len(actions)//4):\n",
    "                action = actions[action_index]\n",
    "                successor_state = state.generateSuccessor(action)\n",
    "                successor_value = self.min_value(successor_state, depth, currentAlpha, beta)[0]\n",
    "                if successor_value > v:\n",
    "                    v = successor_value\n",
    "                    best_action_indices = [action_index]\n",
    "                if successor_value == v:\n",
    "                    best_action_indices+=[action_index]\n",
    "                if v > beta:\n",
    "                    #WHATTTT\n",
    "                    return v, actions[random.choice(best_action_indices)]\n",
    "                currentAlpha = max(currentAlpha, v)\n",
    "            return v, actions[random.choice(best_action_indices)]\n",
    "        \n",
    "    def getAction(self, gameState):\n",
    "        if gameState.currentPlayer == 1 :\n",
    "            return self.max_value(gameState, 0, float(\"-inf\"), float(\"inf\"))[1]\n",
    "        if gameState.currentPlayer == -1 :\n",
    "            return self.min_value(gameState, 0, float(\"-inf\"), float(\"inf\"))[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gameplay(agent):\n",
    "    b = ChessState()\n",
    "#    display(svg.board(b.board, size=100)) \n",
    "#     white = True\n",
    "    for i in range(2):\n",
    "#     while not b.isTerminal():\n",
    "        action = agent.getAction(b)\n",
    "        print(\"move:\", i, action)\n",
    "        b = b.generateSuccessor(action)\n",
    "        #display(svg.board(b.board, size=100)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaBeta starting\n",
      "move: 0 b2b4\n",
      "move: 1 g8h6\n",
      "--- 31.33649468421936 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"AlphaBeta starting\")\n",
    "agent = AlphaBetaAgent()\n",
    "start_time = time.time()\n",
    "gameplay(agent)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta(position, depth, alpha, beta):\n",
    "    \"\"\"Returns a tuple (score, bestmove) for the position at the given depth\"\"\"\n",
    "    if depth == 0 or position.is_checkmate() or position.is_draw():\n",
    "        return (position.evaluate(), None)\n",
    "    else: \n",
    "        if position.to_move == \"white\":\n",
    "            bestmove = None\n",
    "            for move in position.legal_moves():\n",
    "                new_position = position.make_move(move)\n",
    "                score, move = alphabeta(new_position, depth - 1, alpha, beta)\n",
    "                if score > alpha: # white maximizes her score\n",
    "                    alpha = score\n",
    "                    bestmove = move\n",
    "                    if alpha >= beta: # alpha-beta cutoff\n",
    "                        break\n",
    "            return (alpha, bestmove)\n",
    "        else:\n",
    "            bestmove = None\n",
    "            for move in position.legal_moves():\n",
    "                new_position = position.make_move(move)\n",
    "                score, move = alphabeta(new_position, depth - 1, alpha, beta)\n",
    "                if score < beta: # black minimizes his score\n",
    "                    beta = score\n",
    "                    bestmove = move\n",
    "                    if alpha >= beta: # alpha-beta cutoff\n",
    "                        break\n",
    "            return (beta, bestmove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.chessprogramming.org/PeSTO%27s_Evaluation_Function\n",
    "#piece square table, mid game, end game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
